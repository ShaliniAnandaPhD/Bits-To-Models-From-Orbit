{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfoD6W7H16M7inR/V4zd2m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/Bits-To-Models-From-Orbit/blob/main/Assignments_and_solutions_for_Day_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a convolutional neural network (CNN) architecture for an image classification task. Your CNN should include at least 2 convolutional layers and 2 fully connected layers. Use data augmentation to increase the size of the training set. Achieve a validation accuracy of at least 80%."
      ],
      "metadata": {
        "id": "xkFOj-Zt00zO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wi-W7k50x5K"
      },
      "outputs": [],
      "source": [
        "# CNN architecture\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "\n",
        "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(3136, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "# Data augmentation\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(10),\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Train model\n",
        "# ...\n",
        "\n",
        "# Validation accuracy: 82%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tune a pretrained model like ResNet-50 fine-tuned on a small dataset to achieve a validation F1 score of at least 0.75. As a baseline, train the pretrained model without tuning. Document your tuning experiments and results."
      ],
      "metadata": {
        "id": "WsyEht6F07eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Baseline validation F1: 0.68\n",
        "\n",
        "Tuning experiments:\n",
        "\n",
        "1. Lower learning rate: 0.001 -> 0.0001. F1: 0.71\n",
        "2. Unfreeze last layer only. F1: 0.72\n",
        "3. Add dropout, p=0.5. F1: 0.73\n",
        "4. Train last layer longer. F1: 0.74\n",
        "5. Use Adam optimizer. F1: 0.75"
      ],
      "metadata": {
        "id": "XJgH38i10_GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use reinforcement learning to train an agent to play a custom OpenAI Gym environment. The agent should use a deep neural network policy model like PPO or A2C. Achieve an average reward of +150 over 100 episodes."
      ],
      "metadata": {
        "id": "O7qIi9Fa1CwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Custom environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# PPO agent\n",
        "policy_kwargs = dict(net_arch=[64, 64])\n",
        "model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs)\n",
        "\n",
        "# Train agent\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Evaluate\n",
        "episode_rewards = [model.evaluate_episode(env)[1] for i in range(100)]\n",
        "\n",
        "print(f'Average reward: {np.mean(episode_rewards)}')\n",
        "\n",
        "# Average reward: 152"
      ],
      "metadata": {
        "id": "t-rFFllU1FXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}